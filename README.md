ğŸ“Š Customer Churn Prediction â€“ Machine Learning Project
ğŸ” Project Overview

This project focuses on predicting customer churn (whether a client will leave a telecom company) using supervised Machine Learning techniques.
The goal is to build a robust, interpretable and well-validated ML pipeline, from raw data preprocessing to final model evaluation.

The project was developed using Python and scikit-learn, following best practices to avoid data leakage and ensure reproducibility.

ğŸ¯ Objectives

Master core Machine Learning techniques

Apply end-to-end ML pipeline using scikit-learn

Justify every preprocessing decision

Compare multiple ML models fairly

Interpret results from both technical and business perspectives

ğŸ“‚ Dataset Description

Name: Telco Customer Churn

Source: Kaggle

Size: 7,043 rows Ã— 21 columns

Type: Supervised learning â€“ Binary Classification

Target variable: Churn (Yes / No)

ğŸ“Œ Feature Types

Numerical:

tenure, MonthlyCharges, TotalCharges

Categorical:

Gender, Contract type, Internet service, Payment method, etc.

âš ï¸ The dataset is imbalanced (~26.5% churners), which is handled explicitly during modeling.

ğŸ§¹ Data Preprocessing (Fully Justified)

All preprocessing steps are clearly explained and justified inside the notebook.

âœ” Missing Values

TotalCharges converted to numeric

Missing values correspond to new customers (tenure = 0)

Imputed with 0 for business consistency

âœ” Outlier Analysis

Detection using IQR method

Outliers correspond to high-value / long-tenure customers

âŒ No removal to avoid business bias

âœ… Solution: RobustScaler

âœ” Encoding & Scaling

RobustScaler for numerical features

OneHotEncoder

drop='first' â†’ avoids multicollinearity

handle_unknown='ignore' â†’ production-safe

âœ” Data Leakage Prevention

All preprocessing steps applied using scikit-learn Pipelines

fit() only on training data

âš™ï¸ Feature Selection

OneHotEncoding resulted in 5,663 features

Risk of overfitting

Solution:

SelectKBest (ANOVA â€“ f_classif)

Reduced to 30 most informative features

ğŸ¤– Machine Learning Models Used

Seven different models were trained and compared:

Logistic Regression

Random Forest

XGBoost

Support Vector Machine (SVC)

K-Nearest Neighbors (KNN)

Decision Tree

Naive Bayes

âš–ï¸ Model Optimization

5-Fold Cross Validation

Metrics used:

Accuracy

Precision

Recall

F1-score

ROC-AUC

class_weight='balanced' applied when relevant

ğŸ“ˆ Results

ğŸ† Best Model: Logistic Regression

Metric	Score
F1-score	â‰ˆ 0.61
ROC-AUC	â‰ˆ 0.78
Accuracy	â‰ˆ 82%

ğŸ“Œ Logistic Regression was selected for its:

Strong balance between precision and recall

Interpretability

Stability on unseen data

ğŸ“Š Visualizations

The project includes:

Churn distribution plots

Model comparison charts

Confusion matrix

Final LinkedIn-style project infographic

ğŸ“· Example:

ğŸ’¡ Business Insights

Improved churn detection

Reduced false churn alerts

Better targeting of at-risk customers

Clear trade-off between recall and precision

ğŸ›  Technologies Used

Python

scikit-learn

XGBoost

Pandas

NumPy

Matplotlib

Seaborn

Google Colab / Jupyter Notebook

â–¶ï¸ How to Run the Project
pip install -r requirements.txt


Then open the notebook:

jupyter notebook

ğŸ“Œ Author

Fatma Hajjeji
ğŸ“§ Email: fatmahajjeji9@gmail.com

ğŸ”— LinkedIn: https://www.linkedin.com/in/fatma-hajjeji-29b1a8295

â­ Conclusion

This project demonstrates a complete and professional Machine Learning workflow, combining strong technical foundations with clear business interpretation.
It reflects my ability to design, evaluate and explain ML solutions in a real-world context.
