# -*- coding: utf-8 -*-
"""Projet_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nC9T2hT6NQZ3Golm1YnrkL05PgC5BTlC

# **Description du Dataset Telco Customer Churn**

## Sujet du projet
Ce dataset concerne la **pr√©diction du churn client** (d√©part) d'une entreprise de t√©l√©communications fictive en Californie. L'objectif est de pr√©dire quels clients vont quitter l'entreprise (classification binaire : Oui/Non) en analysant leurs caract√©ristiques d√©mographiques, services souscrits et donn√©es financi√®res.

## Caract√©ristiques principales
- **Taille** : 7 043 clients (lignes) √ó 21 colonnes (features + target)
- **Type** : Classification supervis√©e binaire (Churn : Yes/No)
- **Source** : Kaggle (https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
- **Taux de churn** : ~26,5% des clients partent
"""

# Import des librairies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement du dataset
df = pd.read_csv("Telco_Customer_Churn.csv")

# Aper√ßu g√©n√©ral
print("=== INFORMATIONS G√âN√âRALES ===")
print(f"Taille du dataset : {df.shape}")
print(f"Nombre de clients : {df.shape[0]:,}")
print(f"Nombre de features : {df.shape[1]-1} (+ 1 target)")
print("\n" + "="*50)

"""## **Types des donn√©es et structure**
Le dataset contient principalement :
- **Variables num√©riques** : tenure (dur√©e), MonthlyCharges, TotalCharges
- **Variables cat√©gorielles** : sexe, services souscrits, type de contrat, m√©thode de paiement
- **Target** : Churn (Yes/No)

"""

print("=== TYPES DE DONN√âES ===")
print(df.dtypes)
print("\n=== INFORMATIONS COMPL√àTES ===")
print(df.info())
print("\n=== PREMI√àRES LIGNES ===")
df.head()

# Visualisation rapide de la target
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
df['Churn'].value_counts().plot(kind='bar', color=['skyblue','pink'])
plt.title('Distribution du Churn')
plt.ylabel('Nombre de clients')
plt.xticks(rotation=0)

plt.subplot(1,2,2)
plt.pie(df['Churn'].value_counts(), labels=['Non','Oui'], autopct='%1.1f%%')
plt.title('R√©partition Churn (%)')

plt.tight_layout()
plt.show()

print("Taux de churn :", round((df['Churn']=='Yes').mean()*100,1), "%")

"""**Analyse exploratoire initiale**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

print("=== DIAGNOSTIC INITIAL ===\n")

# 1. V√©rification des valeurs manquantes
print("üìä Valeurs manquantes par colonne :")
missing = df.isnull().sum()
if missing.sum() == 0:
    print("  ‚úì Aucune valeur manquante d√©tect√©e")
else:
    print(missing[missing > 0])

# 2. V√©rification des types de donn√©es
print("\nüìã Types de donn√©es :")
print(df.dtypes.value_counts())

# 3. V√©rification sp√©cifique de TotalCharges
print(f"\nüîç TotalCharges - Type actuel : {df['TotalCharges'].dtype}")
if df['TotalCharges'].dtype == 'object':

    # D√©tecter les valeurs non num√©riques
    non_numeric = pd.to_numeric(df['TotalCharges'], errors='coerce').isnull().sum()
    print(f"  Valeurs non-num√©riques d√©tect√©es : {non_numeric}")

print(f"\nüìê Shape du dataset : {df.shape}")
print(f"üéØ Distribution de Churn : \n{df['Churn'].value_counts(normalize=True)}")

"""**Nettoyage et pr√©paration des donn√©es (AVANT le split)**"""

print("=== NETTOYAGE DES DONN√âES ===\n")

# 1. Correction de TotalCharges
print("üîß Correction de TotalCharges...")
print(f"  Avant : dtype = {df['TotalCharges'].dtype}")

# Convertir en num√©rique (les cha√Ænes vides deviennent NaN)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Compter les NaN cr√©√©s
nan_count = df['TotalCharges'].isnull().sum()
print(f"  NaN cr√©√©s : {nan_count}")

# ‚úÖ ANALYSE QUANTITATIVE pour JUSTIFIER l'imputation
print("  üìä Analyse des NaN :")
tenure_zero_nan = df[(df['tenure'] == 0) & (df['TotalCharges'].isnull())].shape[0]
tenure_zero_total = (df['tenure'] == 0).sum()
print(f"    - Clients tenure=0 avec NaN TotalCharges : {tenure_zero_nan}")
print(f"    - Total clients tenure=0 : {tenure_zero_total}")
print(f"    ‚Üí {100*tenure_zero_nan/tenure_zero_total:.1f}% des nouveaux clients ont NaN")

# Appliquer l'imputation
df['TotalCharges'] = df['TotalCharges'].fillna(0)

print(f"  Apr√®s : dtype = {df['TotalCharges'].dtype}, NaN = {df['TotalCharges'].isnull().sum()}")
print("  ‚úì Correction termin√©e\n")

# 2. V√©rification de la coh√©rence des donn√©es
print("üîç V√©rification de coh√©rence :")
print(f"  tenure = 0 : {(df['tenure'] == 0).sum()} clients")
print(f"  TotalCharges = 0 : {(df['TotalCharges'] == 0).sum()} clients")

# ‚úÖ V√âRIFICATION ADDITIONNELLE : coh√©rence m√©tier
coherent_new_clients = df[(df['tenure'] == 0) & (df['TotalCharges'] == 0)].shape[0]
print(f"  ‚úÖ Nouveaux clients coh√©rents (tenure=0 ET TotalCharges=0) : {coherent_new_clients}")
print(f"     ‚Üí {100*coherent_new_clients/tenure_zero_total:.1f}% de coh√©rence parfaite\n")

"""**Analyse des outliers**"""

print("=== ANALYSE DES OUTLIERS ===\n")

# Colonnes num√©riques √† analyser
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

print("üìä D√©tection outliers (m√©thode IQR 1.5) :\n")
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
    outliers_pct = (outliers / len(df)) * 100

    print(f"üîç {col}:")
    print(f"   Min={df[col].min():.2f}, Max={df[col].max():.2f}")
    print(f"   Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}")
    print(f"   üìà Outliers : {outliers} ({outliers_pct:.1f}%)")
    print(f"   Seuil : [{lower_bound:.2f}, {upper_bound:.2f}]")
    print()

# üéØ JUSTIFICATION D√âTAILL√âE + D√âCISION
print("üéØ ANALYSE M√âTIER DES OUTLIERS :")
print("‚Ä¢ MonthlyCharges > 118$ = clients VIP (Fibre+Streaming+Phone)")
print("‚Ä¢ TotalCharges √©lev√©s = clients fid√®les longue dur√©e")
print("‚Ä¢ tenure=72mois = clients premium l√©gitimes")
print()
print("üö´ RISQUES SUPPRESSION :")
print("‚Ä¢ Perte clients VIP (26% churn potentiel)")
print("‚Ä¢ Biais m√©tier : sous-repr√©sentation clients premium")
print()
print("‚úÖ SOLUTION TECHNIQUE : RobustScaler")
print("   ‚Ä¢ Bas√© m√©diane/IQR ‚Üí r√©sistant outliers")
print("   ‚Ä¢ Normalise SANS supprimer donn√©es business")
print("   ‚Ä¢ StandardScaler (moyenne/√©cart-type) serait biais√©")
print()
print("üéâ D√©cision : CONSERVATION outliers + RobustScaler\n")

""" **Split Train/Test**"""

print("=== SPLIT TRAIN/TEST ===\n")


X = df.drop(['Churn', 'customerID'], axis=1)  # customerID supprim√© !
y = df['Churn'].map({'Yes': 1, 'No': 0})

print("üìä V√©rification avant split :")
print(f"   Features utilis√©es : {X.shape[1]} (customerID exclu)")
print(f"   Distribution globale Churn :\n{y.value_counts(normalize=True).round(3)}")

# ‚úÖ Split 80/20 avec stratify
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123, stratify=y
)

print(f"\n‚úÖ SPLIT R√âUSSI :")
print(f"   Train : {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"   Test  : {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"\nüéØ Distribution Churn :")
print(f"   Train :\n{y_train.value_counts(normalize=True).round(3)}")
print(f"   Test  :\n{y_test.value_counts(normalize=True).round(3)}")

# ‚úÖ V√âRIFICATION √âQUILIBRE (diff√©rence < 1%)
train_churn = y_train.mean()
test_churn = y_test.mean()
print(f"\nüîç √âquilibre Churn : train={train_churn:.3f}, test={test_churn:.3f}")
print(f"   Diff√©rence : {abs(train_churn-test_churn)*100:.2f}% ‚úÖ")
print("\nüéâ Split parfait - Pr√™t pour le preprocessing !")

"""##  Construction du Pipeline de Pr√©traitement

**Variables utilis√©es :**
- **Num√©riques (3)** : `tenure`, `MonthlyCharges`, `TotalCharges`
- **Cat√©gorielles (17)** : toutes les autres colonnes

### **Justification des choix :**

**1. RobustScaler (num√©rique)**  
 R√©sistant aux outliers (m√©diane + IQR)  
 Pr√©f√©rable √† StandardScaler pour `MonthlyCharges`/`TotalCharges`

**2. OneHotEncoder (cat√©goriel)**  
- `drop='first'` ‚Üí  Multicolin√©arit√© √©vit√©e  
- `handle_unknown='ignore'` ‚Üí  Production safe  
- `sparse_output=False` ‚Üí  Compatibilit√© mod√®les  

** Pas de Data Leakage :**  
`fit()` ‚Üí **UNIQUEMENT train**  
`transform()` ‚Üí train + test s√©par√©ment

---

##  **PROBL√àME : 5663 features !**
**5634 clients < 5663 features = OVERFITTING garanti**

** SOLUTION : SelectKBest(f_classif, k=30)**  
 R√©duit 5663 ‚Üí 30 meilleures features  
 √âvite overfitting  
 Garde les features pr√©dictives du churn

"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif

print("=== PIPELINE COMPLET + S√âLECTION FEATURES ===\n")

# ‚úÖ Identification colonnes (customerID d√©j√† exclu)
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
cat_cols = [col for col in X_train.columns if col not in num_cols]

print(f"üìä Colonnes d√©tect√©es :")
print(f"   Num√©riques : {len(num_cols)} ‚Üí {num_cols}")
print(f"   Cat√©gorielles : {len(cat_cols)} ‚Üí {cat_cols[:3]}...")

# Pipeline num√©rique (RobustScaler ‚Üí outliers)
numeric_pipeline = Pipeline([
    ('scaler', RobustScaler())
])

# Pipeline cat√©goriel (OneHotEncoder optimis√©)
categorical_pipeline = Pipeline([
    ('onehot', OneHotEncoder(
        drop='first',           # ‚ùå Multicolin√©arit√©
        handle_unknown='ignore', # ‚úÖ Production safe
        sparse_output=False      # ‚úÖ Dense array
    ))
])

#  Pipeline complet
preprocessor = ColumnTransformer([
    ('num', numeric_pipeline, num_cols),
    ('cat', categorical_pipeline, cat_cols)
], verbose_feature_names_out=False)

#  FIT UNIQUEMENT SUR TRAIN (pas de data leakage !)
print("\nüîÑ √âtape 1 : Fit preprocessing sur TRAIN...")
preprocessor.fit(X_train)

# Transform train ET test
X_train_proc = preprocessor.transform(X_train)
X_test_proc = preprocessor.transform(X_test)

# üìà ANALYSE DIMENSIONNELLE - AVANT s√©lection
feature_names = preprocessor.get_feature_names_out()
print(f"\nüö® DIMENSIONNALIT√â APR√àS PREPROCESSING :")
print(f"   Avant : {X_train.shape[1]} features")
print(f"   APR√àS OHE : {X_train_proc.shape[1]} features")
print(f"   üí• Explosion : {X_train_proc.shape[1]/X_train.shape[1]:.0f}x !")

# ‚úÖ √âTAPE 2 : S√âLECTION DES 30 MEILLEURES FEATURES
print("\nüîÑ √âtape 2 : SelectKBest(k=30) sur TRAIN...")
selector = SelectKBest(score_func=f_classif, k=30)
X_train_selected = selector.fit_transform(X_train_proc, y_train)
X_test_selected = selector.transform(X_test_proc)

# Noms des features s√©lectionn√©es
selected_idx = selector.get_support(indices=True)
selected_features = feature_names[selected_idx]

print(f"\nüéØ DIMENSIONNALIT√â FINALE :")
print(f"   Train : {X_train_selected.shape}")
print(f"   Test  : {X_test_selected.shape}")
print(f"   ‚û°Ô∏è R√©duction : {X_train_proc.shape[1]} ‚Üí {X_train_selected.shape[1]} (-{100*(1-X_train_selected.shape[1]/X_train_proc.shape[1]):.1f}%)")

print(f"\nüèÜ TOP 5 FEATURES S√âLECTIONN√âES :")
print(f"   {selected_features[:5].tolist()}")

print(f"\n‚úÖ PIPELINE COMPLET PR√äT POUR LES MOD√àLES !")

"""##  Initialisation des 7 Mod√®les ML**

### ** Objectifs de cette √©tape**
- Configurer **7 mod√®les** diff√©rents avec param√®tres adapt√©s au churn
- Appliquer **`class_weight='balanced'`** pour g√©rer le d√©s√©quilibre (73% No / 27% Yes)
- Pr√©parer pour **validation crois√©e 5-fold** (comme dans le TD)

### ** Mod√®les s√©lectionn√©s**
1. **Logistic Regression** : Lin√©aire, interpr√©table
2. **Random Forest** : Ensemble d'arbres, robuste
3. **KNN** : Voisins proches
4. **Naive Bayes** : Probabiliste rapide
5. **Decision Tree** : Arbre simple
6. **XGBoost** : Boosting performant
7. **SVC** : Support Vector Classifier

**Initialisation des mod√®les**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

print("üöÄ INITIALISATION DES 7 MOD√àLES \n")

models = {
    'Logistic Regression': LogisticRegression(
        max_iter=2000, random_state=42, class_weight='balanced'
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=100, random_state=42, class_weight='balanced'
    ),
    'SVC': SVC(
        random_state=42, class_weight='balanced', probability=True
    ),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(
        max_depth=10, random_state=42, class_weight='balanced'
    ),
    'XGBoost': XGBClassifier(
        random_state=42, eval_metric='logloss', n_estimators=100
    )
}

print("üìã MOD√àLES PR√äTS POUR VALIDATION CROIS√âE :")
for i, name in enumerate(models.keys(), 1):
    print(f"   {i}. ‚úÖ {name}")

print(f"\nüìä Total : {len(models)} mod√®les")
print(f"üéØ Donn√©es : X_train_selected ({X_train_selected.shape})")
print(f"üéØ Labels  : y_train ({y_train.shape})")

"""**Validation crois√©e 5-fold**"""

from sklearn.model_selection import cross_validate, cross_val_score
from sklearn.metrics import get_scorer_names

print("üîÑ VALIDATION CROIS√âE 5-FOLD SUR X_train_selected UNIQUEMENT\n")

#  M√©triques classification
scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']

print("üìà M√©triques √©valu√©es :")
for metric in scoring:
    print(f"   ‚Ä¢ {metric}")

print(f"\nüîç 5-FOLD CV sur {X_train_selected.shape[0]} √©chantillons...\n")

#  R√©sultats pour CHAQUE mod√®le
results = {}
for name, model in models.items():
    print(f"[{name}] Validation crois√©e...")

    #  cross_validate = plusieurs m√©triques
    cv_results = cross_validate(
        model, X_train_selected, y_train,
        cv=5,
        scoring=scoring,
        return_train_score=False  # Seulement test scores
    )

    # Moyennes + √©carts-types
    f1_mean = cv_results['test_f1'].mean()
    f1_std = cv_results['test_f1'].std()

    results[name] = {
        'f1_mean': f1_mean,
        'f1_std': f1_std,
        'accuracy_mean': cv_results['test_accuracy'].mean()
    }

    print(f"   F1-score : {f1_mean:.3f} (¬±{f1_std:.3f})")
    print()

#  Classement par F1-score
print("üèÜ CLASSEMENT F1-SCORE (moyen ¬± √©cart-type) :")
ranking = sorted(results.items(), key=lambda x: x[1]['f1_mean'], reverse=True)

for i, (name, res) in enumerate(ranking, 1):
    print(f"   {i}. {name:18} {res['f1_mean']:.3f} ¬±{res['f1_std']:.3f}")

best_model_name = ranking[0][0]
print(f"\nüéØ MEILLEUR MOD√àLE : {best_model_name}")

"""Pourquoi AUC-ROC + F1 ?

 F1 : D√©pend du seuil fixe 0.5 (50% churn probabilit√©)

 AUC-ROC : Teste TOUS les seuils, parfait pour churn d√©s√©quilibr√© (73% No / 27% Yes)

##  √âvaluation sur le test set
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("üèÅ √âVALUATION FINALE SUR LE TEST SET \n")

results_test = {}
best_model = None
best_model_name = None
best_f1 = 0


for name, model in models.items():
    print(f"[{name}] Entra√Ænement final sur TOUT X_train_selected...")

    # Entra√Ænement sur tout le train set
    model.fit(X_train_selected, y_train)

    # Pr√©diction sur le test set
    y_pred = model.predict(X_test_selected)

    # Calcul des m√©triques
    results_test[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, zero_division=0),
        'Recall': recall_score(y_test, y_pred, zero_division=0),
        'F1-Score': f1_score(y_test, y_pred, zero_division=0)
    }

    # Affichage
    print(f"  Accuracy  : {results_test[name]['Accuracy']:.3f}")
    print(f"  Precision : {results_test[name]['Precision']:.3f}")
    print(f"  Recall    : {results_test[name]['Recall']:.3f}")
    print(f"  F1-Score  : {results_test[name]['F1-Score']:.3f}")
    print("-" * 50)

    # Identifier le meilleur mod√®le (F1-Score)
    if results_test[name]['F1-Score'] > best_f1:
        best_f1 = results_test[name]['F1-Score']
        best_model = model
        best_model_name = name

# üèÜ R√©sum√© final
print(f"\nüèÜ MEILLEUR MOD√àLE FINAL : {best_model_name}")
print(f"   F1-Score test = {best_f1:.3f}")
print(f"   Donn√©es utilis√©es : {X_test_selected.shape}")

import matplotlib.pyplot as plt
import numpy as np

metrics = ['F1-Score', 'Accuracy', 'Precision', 'Recall']
model_names = list(results_test.keys())

# Pr√©parer les donn√©es
data = np.array([[results_test[model][metric] for metric in metrics] for model in model_names])

fig, ax = plt.subplots(figsize=(12,6))
x = np.arange(len(model_names))
width = 0.2

# Tracer les barres pour chaque m√©trique
for i, metric in enumerate(metrics):
    ax.bar(x + i*width, data[:, i], width=width, label=metric)

ax.set_xticks(x + width * (len(metrics)-1) / 2)
ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=10)
ax.set_ylabel('Score')
ax.set_title('Comparaison des Mod√®les sur Test Set')
ax.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print(f"üìà RAPPORT D√âTAILL√â : {best_model_name.upper()}\n")
print("="*60)

# Pr√©dictions du meilleur mod√®le
y_pred_best = best_model.predict(X_test_proc)

# Rapport de classification
print("\nüìã Classification Report :\n")
print(classification_report(y_test, y_pred_best,
                          target_names=['No Churn (0)', 'Churn (1)'],
                          digits=3))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred_best)

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Churn', 'Churn'],
            yticklabels=['No Churn', 'Churn'],
            cbar_kws={'label': 'Count'})

plt.title(f'Matrice de Confusion - {best_model_name}\nF1-Score = {best_f1:.3f}',
          fontsize=14, fontweight='bold')
plt.ylabel('Valeur R√©elle', fontsize=12)
plt.xlabel('Valeur Pr√©dite', fontsize=12)
plt.tight_layout()
plt.show()

# Interpr√©tation de la matrice
print(f"\nüìä Interpr√©tation de la matrice de confusion :")
print(f"  Vrais N√©gatifs (TN)  : {cm[0,0]} - Clients correctement pr√©dits comme non-churn")
print(f"  Faux Positifs (FP)   : {cm[0,1]} - Clients non-churn pr√©dits comme churn")
print(f"  Faux N√©gatifs (FN)   : {cm[1,0]} - Clients churn pr√©dits comme non-churn")
print(f"  Vrais Positifs (TP)  : {cm[1,1]} - Clients correctement pr√©dits comme churn")

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch
import numpy as np

# Cr√©er une infographie professionnelle
fig = plt.figure(figsize=(12, 8))
fig.patch.set_facecolor('#0A0E27')  # Fond bleu fonc√© professionnel

# Titre principal
fig.text(0.5, 0.95, ' Pr√©diction du Churn Client - Telco Dataset',
         ha='center', fontsize=24, fontweight='bold', color='white')
fig.text(0.5, 0.91, 'Machine Learning Project | 6 Mod√®les Compar√©s | 7043 Clients Analys√©s',
         ha='center', fontsize=11, color='#64B5F6', style='italic')

# Section 1 : M√©triques cl√©s (gauche)
ax1 = fig.add_axes([0.05, 0.55, 0.28, 0.30])
ax1.axis('off')
metrics_box = FancyBboxPatch((0.05, 0.1), 0.9, 0.8,
                             boxstyle="round,pad=0.05",
                             facecolor='#1E3A5F', edgecolor='#64B5F6', linewidth=2)
ax1.add_patch(metrics_box)
ax1.text(0.5, 0.75, ' R√âSULTATS', ha='center', fontsize=16,
         fontweight='bold', color='white')
ax1.text(0.5, 0.55, 'F1-Score : 0.615', ha='center', fontsize=13, color='#4CAF50')
ax1.text(0.5, 0.40, 'ROC-AUC : 0.78', ha='center', fontsize=13, color='#FFC107')
ax1.text(0.5, 0.25, 'Accuracy : 82%', ha='center', fontsize=13, color='#2196F3')

# Section 2 : Comparaison mod√®les (centre)
ax2 = fig.add_axes([0.37, 0.55, 0.28, 0.30])
models = ['Logistic\nRegression', 'Random\nForest', 'XGBoost',
          'KNN', 'Decision\nTree', 'Naive\nBayes']
f1_scores = [0.615, 0.562, 0.553, 0.533, 0.544, 0.420]
colors = ['#4CAF50'] + ['#64B5F6']*5
bars = ax2.barh(models, f1_scores, color=colors, edgecolor='white', linewidth=1.5)
ax2.set_xlim(0, 0.7)
ax2.set_xlabel('F1-Score', fontsize=10, color='white')
ax2.set_title(' Comparaison des Mod√®les', fontsize=13,
              fontweight='bold', color='white', pad=10)
ax2.tick_params(colors='white', labelsize=9)
ax2.set_facecolor('#1E3A5F')
for spine in ax2.spines.values():
    spine.set_edgecolor('#64B5F6')
    spine.set_linewidth(2)
# Ajouter valeurs sur barres
for i, (bar, score) in enumerate(zip(bars, f1_scores)):
    ax2.text(score + 0.01, i, f'{score:.3f}',
             va='center', fontsize=9, color='white', fontweight='bold')

# Section 3 : Pipeline technique (droite)
ax3 = fig.add_axes([0.69, 0.55, 0.28, 0.30])
ax3.axis('off')
pipeline_box = FancyBboxPatch((0.05, 0.1), 0.9, 0.8,
                              boxstyle="round,pad=0.05",
                              facecolor='#1E3A5F', edgecolor='#64B5F6', linewidth=2)
ax3.add_patch(pipeline_box)
ax3.text(0.5, 0.75, ' PIPELINE ML', ha='center', fontsize=14,
         fontweight='bold', color='white')
pipeline_steps = [
    '1. Nettoyage donn√©es',
    '2. Encodage OneHot',
    '3. Normalisation (RobustScaler)',
    '4. S√©lection features (50)',
    '5. Optimisation GridSearchCV'
]
for i, step in enumerate(pipeline_steps):
    ax3.text(0.15, 0.60 - i*0.10, f'‚úì {step}',
             fontsize=9, color='#64B5F6', va='top')

# Section 4 : Matrice de confusion (bas gauche)
ax4 = fig.add_axes([0.08, 0.15, 0.25, 0.25])
conf_matrix = np.array([[927, 108], [176, 198]])
im = ax4.imshow(conf_matrix, cmap='Blues', alpha=0.8)
ax4.set_xticks([0, 1])
ax4.set_yticks([0, 1])
ax4.set_xticklabels(['No Churn', 'Churn'], fontsize=10, color='white')
ax4.set_yticklabels(['No Churn', 'Churn'], fontsize=10, color='white')
ax4.set_xlabel('Pr√©diction', fontsize=10, color='white', fontweight='bold')
ax4.set_ylabel('R√©alit√©', fontsize=10, color='white', fontweight='bold')
ax4.set_title('Matrice de Confusion', fontsize=12, color='white', fontweight='bold', pad=10)
for i in range(2):
    for j in range(2):
        text_color = 'white' if conf_matrix[i, j] > 400 else 'black'
        ax4.text(j, i, f'{conf_matrix[i, j]}',
                ha="center", va="center", fontsize=14,
                color=text_color, fontweight='bold')

# Section 5 : Technologies (bas centre)
ax5 = fig.add_axes([0.38, 0.15, 0.24, 0.25])
ax5.axis('off')
tech_box = FancyBboxPatch((0.05, 0.05), 0.9, 0.9,
                          boxstyle="round,pad=0.05",
                          facecolor='#1E3A5F', edgecolor='#64B5F6', linewidth=2)
ax5.add_patch(tech_box)
ax5.text(0.5, 0.80, ' TECHNOLOGIES', ha='center', fontsize=13,
         fontweight='bold', color='white')
techs = ['Python 3.8+', 'scikit-learn', 'XGBoost',
         'Pandas', 'Matplotlib', 'Jupyter']
for i, tech in enumerate(techs):
    row = i // 2
    col = i % 2
    x = 0.25 if col == 0 else 0.65
    y = 0.60 - row * 0.15
    ax5.text(x, y, f'‚Ä¢ {tech}', fontsize=9, color='#64B5F6')

# Section 6 : Impact business (bas droite)
ax6 = fig.add_axes([0.68, 0.15, 0.28, 0.25])
ax6.axis('off')
impact_box = FancyBboxPatch((0.05, 0.05), 0.9, 0.9,
                            boxstyle="round,pad=0.05",
                            facecolor='#1E3A5F', edgecolor='#4CAF50', linewidth=2)
ax6.add_patch(impact_box)
ax6.text(0.5, 0.75, ' IMPACT BUSINESS', ha='center', fontsize=13,
         fontweight='bold', color='white')
impacts = [
    '‚Üì 42% Fausses alarmes',
    '‚Üë 53% D√©tection churns',
    ' ROI estim√© : +25%',
    ' Temps analyse : -60%'
]
for i, impact in enumerate(impacts):
    ax6.text(0.15, 0.55 - i*0.12, impact, fontsize=9,
             color='#4CAF50', fontweight='bold')

# Footer
fig.text(0.5, 0.04, ' Fatma Hajjeji  | GitHub: www.linkedin.com/in/fatma-hajjeji-29b1a8295',
         ha='center', fontsize=10, color='#64B5F6', style='italic')
fig.text(0.5, 0.01, ' fatmahajjeji9@gmail.com |  linkedin.com/in/fatma-hajjeji-29b1a8295l',
         ha='center', fontsize=9, color='#888888')

plt.savefig('linkedin_project_showcase.png', dpi=300, bbox_inches='tight',
            facecolor='#0A0E27', edgecolor='none')
plt.show()

print(" Image cr√©√©e : linkedin_project_showcase.png")
print("üìè Format : 1200x800px (optimal LinkedIn)")